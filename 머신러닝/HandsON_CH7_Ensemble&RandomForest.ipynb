{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7625025a",
   "metadata": {},
   "source": [
    "### 앙상블과 랜덤포레스트\n",
    "- 대중의 지혜를 이용한 모델\n",
    "- 일련의 예측기로부터 예측을 수행하면 더 좋은 예측을 얻을 수 있을 것에서 고안한 모델\n",
    "- 랜덤 포레스트 이외에도 배깅, 부스팅, 스태킹 등이 있음\n",
    "- 가능한 모델들이 서로 독립적일 때 최고의 성능을 발휘함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207eaa7",
   "metadata": {},
   "source": [
    "##### 투표기반 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c3425d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data'][:,2:]\n",
    "y = iris['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4228b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9666666666666667\n",
      "RandomForestClassifier 0.9666666666666667\n",
      "SVC 0.9666666666666667\n",
      "VotingClassifier 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 정확도 확인\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5bdc8",
   "metadata": {},
   "source": [
    "##### cf) 간접투표\n",
    "- 모든 분류기에 predict_proba()함수가 있다면,\n",
    "- voting='soft'로 두면, 확률값의 평균이 가장 높은 클래스를 선택하는 식으로 작동함\n",
    "- 당연히 성능은 더욱 좋아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339fe01f",
   "metadata": {},
   "source": [
    "##### 배깅과 페이스팅\n",
    "- 앙상블은 훈련세트의 서브셋을 무작위로 구성하여 분류기를 각기 다르게 학습시키는 것\n",
    "- 훈련세트에서 중복을 허용하여 샘플링하는 방식을 배깅이라함\n",
    "- 중복을 허용하지 않고 샘플링하는 방식을 페이스팅이라고 함\n",
    "- 훈련을 마친 예측을 모으는 수집함수는 전형적으로 분류일 때는 '통계적 최빈값'을 사용함\n",
    "- 수집함수를 통과하면 편향과 분산이 모두 감소하는 효과를 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09186b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, # 몇 개의 트리분류기를 앙상블 시킬 것인지\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1 # bootstrap=False로 주면 페이스팅\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5695450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f4773",
   "metadata": {},
   "source": [
    "##### 랜덤 포레스트\n",
    "- 작동방식은 결정트리를 배깅에 넣어 활용하는 것과 같음\n",
    "- 랜덤 포레스트는 결정트리에 최적화되어 사용하기 편리하게 만든 것\n",
    "- 특성 중요도 : 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56267d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_jobs 매개변수를 -1로 주어 시스템 CPU 코어 수를 모두 사용해 학습함\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3907fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d772903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a25d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08955652834134575\n",
      "sepal width (cm) 0.020390659794703666\n",
      "petal length (cm) 0.46427830629272615\n",
      "petal width (cm) 0.42577450557122454\n"
     ]
    }
   ],
   "source": [
    "# feature_importances_로 특성별 중요도 파악 가능\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117aab2",
   "metadata": {},
   "source": [
    "##### 부스팅\n",
    "- AdaBoost, gradient boosting 등이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221fc15",
   "metadata": {},
   "source": [
    "##### AdaBoost\n",
    "- 이전 모델이 과속적합했던 훈련 샘플의 가중치를 더 높이는 방식\n",
    "- 첫 번째 분류기를 훈련세트에서 훈련시키고 예측을 만듦\n",
    "- -> 그 다음 잘못 분류한 훈련샘플의 가중치를 상대적으로 높임\n",
    "- -> 그 다음... 반복\n",
    "- 경사 하강법이 비용함수를 최소화하기 위해 파라미터를 조정하는 것처럼\n",
    "- AdaBoost는 점차 더 좋아지도록 앙상블에 예측기를 추가하는 방식\n",
    "- 가장 큰 단점은 병렬적 학습을 수행할 수 없다는 것(이전 학습이 완료되어야 가중치를 추가하므로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46da069d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm='SAMME.R', learning_rate=0.5\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2290fae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = ada_clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f58004",
   "metadata": {},
   "source": [
    "##### 그레디언트 부스팅\n",
    "- AdaBoost와 기본적인 원리는 같지만, 샘플의 가중치를 수정하는 것이 아니라,\n",
    "- 이전까지의 예측기가 만든 잔여오차에 새로운 예측기를 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af35e888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a53f483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 예측기에서 생긴 잔여오차에 두번째 결정트리회귀를 훈련시킴\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb80edfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51cae36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 샘플에 대한 예측을 만드려면 모든 트리의 예측을 더하면 됨\n",
    "y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3e81c1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90d09131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827942142434103"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과소적합\n",
    "gbrt.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c8db06d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=200)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28786fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916712978247452"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과대적합\n",
    "gbrt.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f189aea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=30)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적의 트리수를 찾기위해 조기종료기법 사용하기\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)+1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0c0c4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9744706544206438"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dbb5e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률적 그레이디언트 부스팅\n",
    "# subsample=0.25라고 지정하면 트리는 무작위로 선택된 25%의 훈련샘플로 학습됨\n",
    "# 편향이 높아지는 대신 분산이 낮아지게되며, 훈련 속도를 상당부분 개선시킬 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e2e47",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "- 최적화된 그레이디언트 부스팅\n",
    "- 빠른속도, 이식성, 확장성이 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bca0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b0379",
   "metadata": {},
   "source": [
    "##### 스태킹\n",
    "- 앙상블 모델에 예측을 취합하는 함수를 사용하는 대신, 취합하는 모델을 훈련시키는 방법을 사용하는 것\n",
    "- 함수 대신 사용하는 학습기를 블랜더 혹은 메타학습기라고 부름\n",
    "- sklearn에 직접적인 모델이 있는 것이 아님, 직접 구현하면 됨(간단)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443eaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
